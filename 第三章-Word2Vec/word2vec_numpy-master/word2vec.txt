# In this lesson we are going to talk about using code to realize word2vec.
# 代码参考了DerekChia老师在github上的一个项目，分三部分，
# 第一部分，读取数据、构造单词Embedding和One-Hot向量。
# 第二部分，根据word2vec的思想构造训练数据。
# 第三部分，构造前向/后向传播函数和softmax函数对数据进行训练。
# 第四部分，获取词向量，构造相似度函数，计算单词之间的相似度。
# DerekChia老师这个项目里的源代码主要使用了Python的类，我把它修改成了许多函数，这样运行和看起来会比较方便。
# 我们导入了两个模块，一个是大家比较熟悉的numpy，另一个是从collections中导入了defaultdict这个方法
# 这是一个构造数据字典的方法，具体的使用方式我们一会儿会进行介绍
# 首先，第一小节，读取数据，我们先定义两个字符串，也就是两句话，作为输入
# text1——自然语言处理和机器学习非常有趣，text2——自然语言处理和python是非常有意思的。
# 然后把两句话放到一个列表里，如果我们有更多数据，texts这个列表就保存了许多句的文本内容。现在这里只有两个。
# 然后我们新建一个corpus列表，这个列表首选循环所有texts列表中的所有句子
# 对于每句话，也就是每个text，对其做for循环将所有单词按空格分割开，再把所有的大写单词转化为小写，保存在一个列表中。
# 所以我们看这些corpus这个列表，里面包含了两个列表，每个列表是一句话，里面的每个列表就是每句话的所有转成小写的单词。
# 我们现在可以定义和调用这个read_data函数，得到corpus数据。
# 第二小节，我们来看怎么构造单词的Embedding
# 这里的Embedding是狭义上的Embedding，可以认为是一种唯一索引的Embedding
# 来看一下它的构造过程
# 首先，我们调用之前从collections导入的defaultdict模块，定义一个defaultdict对象word_counts
# 来看一下，word_counts现在是一个defaultdict对象，具有int属性，取值为空字典
# 接下来，我们循环corpus列表，对于它的每个元素，也就是每个列表，再进行for循环，取其中的每个单词
# 然后让word_counts这个对象每次累加1
# 运行玩这些代码后，我们看一下word_counts现在变成了什么
# 可以看到，word_counts原来的空字典变成了这样一个字典
# 这个字典的意思呢，就是相当于把corpus里的所有数据
# 或者我们输入的文本的所有单词，统计了它们出现的个数
# 例如我们可以数一下，原来的两句话中有4个and
# 一个，两个，三个，四个，原来的两句话中有两个is，一个，两个，这里
# 字典构造完成后，我们定义一个v_count，保存我们这个字典中的所有key的个数
# 实际上，由于我们的字典word_counts保存的是所有数据的唯一单词及其出现的个数
# 那么v_count相当于整个字典的长度，或者说所有唯一单词的总个数
# 我们来看一下，结果是11
# 我们再定义一个words_set列表，保存作为所有唯一单词的集合
# 实际上，v_count的大小就是words_set的长度
# 可以看一下，words_set和word_counts这一列的取值是一样的
# 它的总单词数也是11
# 最后，我们在对words_set这个列表进行for循环
# 对列表使用enumerate函数再进行for循环，能够把每个元素的值和它在列表中的index取出来
# 我们的word_index在for循环时保存的是以单词word为键，以index，i为值的字典
# index_word正好相反，是以index，i为键，以单词word为值
# 我们运行两行代码分别看一下这两个字典的结构
# 实际上相当于这两列调换了位置，所以它们的名字也正好相反
# 第三小节我们讲解如何构造One-Hot向量
# 我们定义这样一个word2onehot的函数，传入单词，ｖ_count和word_index三个变量
# 假设我们传入的单词是and，在这里，我们定义word="and"
# 首先，我们预定义一个word_vec的列表
# 它是根据v_count来定义的，也就是我们唯一单词的总数量来定义
# 我们看到，现在它是一个全0的列表，里面有v_count长度个0，现在也就是11个0
# 我们定义word_ix等于word_index这个字典中以传入的word为key的值
# 现在word="and"这个单词，在word_index这个字典中，我们发现它的值为３
# 也就是现在我们的word_ix=3，紧接着，我们令word_vec这个列表的第word_ix个元素，
# 也就是第４个元素，赋值为1
# 来看一下现在这个word_vec，现在它在第四个位置上取值为１，其他元素取值为０
# 有同学有些疑问为什么不是第３个位置取值为１，因为我们word_index字典的索引是从０开始取的
# 因此，我们的word2onehot函数的作用是什么呢，
# 当我们传入一个单词，根据这个单词在我们所维护的word_embedding，也就是word_index字典中的位置，
# 需要我们输出一个向量，向量的长度等于word_embedding字典的长度
# 向量中的元素，除了我们传入的单词在对应位置上为１，其他位置全部为０
# 再比如这里，假设我们传入的是fun这个单词，
# 我们看到，输出的word_vec在第８个单词的位置取值为１，其他取值为０
# 可以想象一下，如果我们的语料库很大，不重复单词的个数很多，
# 多到类似于我们在现实中买到的格林英语词典中的单词这么多，
# 实际上我们的word_index或者index_word就是对这个词典中的所有单词构建了一个python字典
# 假设有20000个单词，那么我们的one-hot向量的长度就是20000
# 相当于每个单词都有一个自己的one-hot向量，这样如果再给定一句话，
# 就是说一句话有多少个单词，就有多少个20000长度的one-hot向量，
# 那么这样，一句话就可以认为是多个向量组成的矩阵了。
# 多句话就是多个矩阵，也就是三维张量的概念了。
# 这里不理解的同学可以再仔细体会一下。
# 讲完这些内容后，我们讲解第二部分，根据word2vec的思想构造训练数据。
# 首先来回顾一下word2vec构造周围词和目标词的方式，
# 在CBOW模型中，输入是某个单词的周围词，输出目标词是周围词中间的当前词。
# 在Skip-Gram模型中，情况正好相反。
# 我们定义get_training_data这样一个函数，传入三个参数
# corpus,v_count和word_index，都是我们之前比较熟悉的
# 首先，定义一个空列表training_data
# 对corpus中的每句话——也就是每个列表进行循环
# 定义循环到的这句话的长度为sent_len
# 这里我们以corpus的第一个元素为例，sentence就是我们现在这个列表
# 现在这句话里有10个单词，因此我们的sent_len＝10
# 继续对这句话的每个单词进行循环，我们使用enumerate函数同时取出循环到单词的索引i和单词word
# 为了方便讲解，我们也假设取i=3,也就是word=“and”时的数据
# 我们定义w_target目标词是sentence的第i+1个单词（索引从0开始），
# 也就是“and”这个单词，使用word2onehot函数，得到and这个单词的one-hot向量作为w_target
# 可以看到，w_target这个结果是我们刚刚见过的。
# 然后我们定义一个周围词或者上下文词的空列表，
# for循环是从i-2一直循环到i+2+1,注意到我们现在的i是等于3的，
# 就是说，for循环是从3-2=1循环到3+2+1=6,就是1到6
# 我们打印出每个j的结果，可以看到是1/2/3/4/5
# 需要注意的是，这里的i-2和i+2+1里的2，指的是word2vec滑窗技术中的窗口的大小，后面我们会再介绍
# 紧接着，我们的for循环下面有个if条件进行约束，
# 只有当j大于等于0，j不等于i并且j小于句子长度sent_len-1时才进行下面的操作
# 注意到，现在我们的i是等于3的。所以在if条件的限制下，就只有1,2,4,5了，排除了3的情况，
# 我们再打印一下，看一下结果，1,2,4,5
# 在i取值小于2和大于最大长度-2时，情况有所不同，
# 假设我们的i=0, 如果不加if条件，我们先打印一下j，可以看到取值为-2,-1,0,1,2
# 加上if条件的限制，我们发现除了排除了当前的i=0，j>=0的限制也排除了-2,-1。因此只剩下了1,2
# 为了更加清晰地理解这种滑窗构造数据的过程，我们打开DerekChia老师的github或者博客，
# 我们看到这里第一张表，从上往下一共有10行，也就是我们代码中的第二层for循环的次数，
# 或者说我们句子中所有单词的总个数。
# 我们来看第一行，这里就是i=0的情况，也就是我们的w_target现在是natual这个单词
# 刚刚我们分析过，i=0时，第三层for循环加上if条件的限制，最终只剩下1和2，
# 在这个图中，也就是我们的后面两个单词language和processing
# i=3时，也就是来看第4行，and单词作为w_target，
# 周围的四个单词language、processing,mechine和learning作为周围词w_context
# i=1, i=2, i=3，一直到最后，每次循环构造的数据就如图所示。
# 继续回到我们的代码中，我们发现当if条件满足时，
# w_context就定义为所有符合条件的句子中的第j个词的one-hot向量，
# 那么由于我们的w_contexts，是在第二层for循环运行到第i个单词的时候定义的
# 第三层for循环可以说是根据i来构造的周围词数据，
# 因此w_contexts在第三层for循环时append每一个周围词w_context（末尾少了个s）
# 所以可以看到，我们一个i对应一个w_target,对应一个w_contexts，也就是一组周围词
# 我们的training_data在每次完成第i词循环以后，同时append了w_target和w_contexts
# 加上第一层for循环，所以最终training_data保存了所有句子中的所有当前词和周围词的数据
# 最后用np.array将training_data列表转换成了numpy的array数据
# 我们定义函数并整体调用一下，看一下traning_data最后的结构
# 首先，我们发现一共有19条数据，对应于我们原始输入数据中的两句话所有单词的总个数。
# 打开看一下，第一列就是所有这19个单词每个单词的one-hot向量。
# 第二列，对应于第一列的每一个单词，就是这个单词的滑动窗口为2的周围词的one-hot向量
# Ok，到这里的话，我们的训练数据就构造好了。
# 关于训练数据one-hot的表示如果大家有不太懂的也可以结合DerekChia老师的第二张图来进行理解
# 第二张图与第一张展示的本质内容是一样的，区别是第二张图使用了更详细的one-hot向量图进行了展示
# 这一小节的视频我们就讲这么多，下一节我们将继续讲解后面的内容。谢谢大家。


